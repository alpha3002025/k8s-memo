# 2.10.controlplane - scheduler

강의에서 첨부한 자료가 없어서 직접 추가
![](https://kubernetes.io/images/docs/kubernetes-cluster-architecture.svg)

<br/>

출처 : https://kubernetes.io/docs/concepts/architecture/
- control plane 검색하면 나옴
- 또는 https://kubernetes.io/docs/concepts/overview/components/ 에서 설명하는 그림 역시 좋다.

<br/>
<br/>

**Scheduler**<br/>
Control plane 내에서 중요한 역할을 수행합니다. 스케쥴러는 클러스터 내 가용 리소스(CPU, 메모리 등)와 파드의 요구
사항을 고려하여, 적절한 노드에 파드를 배치합니다.

파드 생성시 스케쥴러는 다음과 같이 동작합니다.
- "스케쥴 되지 않은 상태"로 대기 (Pending)
- 스케쥴러가 감지 후 적절한 노드에 할당

파드 생성/배치 시 스케쥴러가 고려하는 요소는 다음과 같습니다.
- 리소스 사용량
- 워크로드 균형
- 노드 상태 (가용성, 레이블 등)

<br/>
<br/>


중간에 EC2 설명을 하는데 EC2 cpu 아키텍쳐선택시 x86, arm cpu 선택하는 메뉴를 보고 흥분해서 부가지식 설명해줬는데 graviton cpu 에 대해 설명하기 시작하셨다. 결론적으로 ARM 기반의 graviton cpu 가 가성비가 엄청 좋다고 한다.<br/>
<br/>

아래에서부터는 gemini 와 함께 강의 내용에 대해 요약할 부분은 요약하고, 질문도 하고, 자료도 찾고 하면서 정리한 내용이다.
로컬 파일에는 k8s-notes/fastcampuse-k8s/2.10-scheduler-and-controlplane.md 에 원본으로 저장해두었다.<br/>
<br/>


# The Role of scheduler in Kubernetes (쿠버네티스 스케줄러의 역할)
<br/>
  

## 1. Assigns Pods to Nodes (파드를 노드에 할당)

쿠버네티스 스케줄러는 리소스 요구 사항, 가용성, 제약 조건 등 다양한 요소에 따라 **새로 생성된 Pod를 클러스터 내의 적합한 노드에 할당**하는 역할을 합니다.<br/>

단순히 아무 노드에나 두는 것이 아니라, 다음의 **2단계 결정 과정**을 거칩니다.<br/>
<br/>
  

### 1단계: Filtering (필터링 - 자격 심사)

- "이 Pod가 들어갈 수 있는 노드는 어디인가?"를 판단하여 후보군을 추립니다.

- 예: "CPU 2 Core가 필요한데, 남은 공간이 있는 노드는?" -> Node A, Node B 통과, Node C 탈락.

- 만약 이 단계에서 남는 노드가 하나도 없으면 Pod는 `Pending` 상태로 대기합니다.

<br/>
<br/>

### 2단계: Scoring (스코어링 - 점수 매기기)

- 남은 후보 노드 중 "어디가 **가장** 좋은가?"를 점수로 매겨 1등을 뽑습니다.

- 다양한 함수(Scoring Functions)를 통해 점수를 계산합니다.

- 예: *ImageLocality* (이미지가 이미 다운로드된 노드에 가산점), *LeastRequested* (자원이 널널한 곳을 선호).

- 최종적으로 1등 노드에 Pod를 **Binding** 합니다.

<br/>
<br/>

## 2. Considers Constraints (제약 조건 고려)

스케줄러는 스케줄링 결정을 내릴 때 다음과 같은 다양한 제약 조건과 기본 설정을 고려하여 최적의 위치를 선정합니다.

<br/>

### Affinity & Anti-affinity (친화성 및 반친화성)

- **Node Affinity**: 특정 라벨이 있는 노드를 선호합니다. (예: `gpu=true`인 노드에만 가고 싶어!)

- **Pod Affinity**: 특정 Pod가 있는 곳에 같이 가고 싶어 합니다. (예: 웹 서버와 캐시 서버는 같은 노드에 있는 게 빨라.)

- **Pod Anti-Affinity**: 특정 Pod와는 떨어지고 싶어 합니다. (예: 마스터 DB와 슬레이브 DB는 물리적으로 다른 노드에 둬야 안전해.)

<br/>
<br/>


### Taint & Tolerations (얼룩과 관용)

- **Taint (노드에 설정)**: "나한테 오지 마!"라고 거부하는 설정입니다. (예: 마스터 노드에는 일반 Pod가 못 오게 설정됨.)

- **Toleration (파드에 설정)**: "나는 그 얼룩(Taint)을 견딜 수 있어"라는 출입증입니다. 이 설정이 있어야만 Taint된 노드에 배치될 수 있습니다.

<br/>
<br/>


## 3. Monitors Node Capacity (노드 용량 모니터링)

스케줄러는 각 노드에서 **사용 가능한 리소스(CPU, Memory, Disk 등)를 지속적으로 모니터링**합니다.

<br/>

### Resource Request 기반 판단

- 스케줄러는 현재 노드의 '실제 사용량'이 아니라, 파드들이 예약한 **'요청량(Requests)의 합'**을 기준으로 판단합니다.

- **공식**: `Node Capacity` - `System Reserved` - `Existing Pods Requests` = **Allocatable (할당 가능량)**

- 새로 들어올 Pod의 Request가 이 Allocatable보다 크면, 자리가 부족하다고 판단하여 배치하지 않습니다.

<br/>
<br/>
  

## 4. Supports Extendability (확장성 지원)

쿠버네티스 스케줄러는 **확장 가능하도록 설계**되어 있습니다. 사용자가 기본 로직 외에 자신만의 규칙을 추가할 수 있습니다.

<br/>

### Scheduling Framework (스케줄링 프레임워크)

- 스케줄러의 동작 과정을 여러 단계(QueueSort -> PreFilter -> Filter -> Score -> Bind)로 나누고, 각 단계마다 **플러그인(Plugin)**을 끼워 넣을 수 있게 만들었습니다.

- 사용자는 Go 언어로 커스텀 플러그인을 작성하여 컴파일하면, 나만의 스케줄러를 만들 수 있습니다.

- **활용 예시**:

- AI 워크로드를 위해 GPU 메모리 파편화를 최소화하는 스케줄러.

- **Gang Scheduling**: 연관된 Pod 10개가 한꺼번에 뜰 자리가 없으면, 1개도 띄우지 않고 기다리게 하는 방식.

<br/>
<br/>


## 참고 (1). Autoscaling & Dynamic Provisioning (오토스케일링 및 동적 프로비저닝)

스케줄러와 밀접하게 연동하여 클러스터의 자원을 자동으로 조절하는 핵심 컴포넌트들입니다.

<br/>

### 1. HPA (Horizontal Pod Autoscaler)

- **개념**: Pod의 부하(CPU, Memory 등)가 늘어나면 **Pod의 개수(Replicas)**를 자동으로 늘리는 컨트롤러입니다.

- **역활**: "일꾼이 힘들어하네? 일꾼 더 투입해!" (Scale Out)

- **스케줄러와의 관계**: HPA가 Pod 개수를 늘리면, 스케줄러는 새로 생긴 Pod(Pending)를 노드에 배치합니다.

<br/>
<br/>


### 2. CA (Cluster Autoscaler)

- **개념**: 스케줄러가 Pod를 배치하려는데 **자리가 부족(Pending)할 때, 노드(VM instance) 자체를 추가**하는 컴포넌트입니다.

- **역할**: "일꾼을 늘리려는데 사무실이 꽉 찼네? 사무실을 확장하자!" (Node Scale Out)

- **작동 방식**: AWS Auto Scaling Group(ASG)등과 연동하여 서버 대수를 늘립니다. 비교적 반응 속도가 느리고(수 분 소요), 설정이 복잡합니다.

<br/>
<br/>
  

### 3. Karpenter (카펜터 - Next Gen Node Provisioner)

- **개념**: AWS가 개발한(오픈소스) 차세대 노드 오토스케일러로, CA의 단점을 해결하기 위해 등장했습니다.

- **특징**:

- **No Wait**: CA처럼 주기적으로 체크하는 게 아니라, Pending Pod가 생기자마자 즉시(Real-time) 반응합니다.

- **Just-in-Time Provisioning**: Pod의 요구사항(CPU, GPU, Arch 등)을 분석하여, **"딱 맞는 사이즈"**의 인스턴스를 골라서 띄워줍니다. (예: 작은 Pod면 t3.small, 큰 Pod면 m5.large를 선택)

- **빠른 속도**: 노드 부팅부터 Pod 배치까지 수십 초 내에 완료됩니다.

- **Scheduling**: 스케줄러가 노드를 찾지 못해 Pending 상태가 되면, Karpenter가 개입하여 **"맞춤형 노드"**를 생성하고 바로 바인딩(Binding)까지 도와줍니다.

<br/>
<br/>

## 부가자료 (1). Emerging Hardware Trends: Custom Silicon (최신 하드웨어 트렌드: 독자 반도체)

클라우드 환경에서 노드(Node) 선택 시, 비용 효율성과 성능 최적화를 위해 x86(Intel/AMD) 아키텍처 대신 ARM 기반 프로세서의 도입이 가속화되고 있습니다.<br/>
<br/>


### AWS Graviton 5 (2025 Release)

AWS가 자체 개발한 ARM 기반 프로세서의 최신 버전으로, 클라우드 워크로드에 초적화된 성능과 효율성을 제공합니다.

- **주요 스펙**:

- **공정 및 코어**: 3nm 공정 적용, 단일 소켓에 **192개 코어** 탑재 (Graviton 4 대비 2배 수준의 코어 집적도).

- **메모리**: DDR5-8400 메모리를 지원하여 대역폭이 획기적으로 증가.

- **성능**: 전작 Graviton 4 대비 최대 25% 성능 향상 및 에너지 효율성 개선.

- **특징**:

- **단일 소켓 디자인**: 기존의 듀얼 소켓 구성을 단일 소켓으로 통합하여 코어 간 통신 지연(Latency)을 최소화했습니다.

- **L3 캐시**: 5배 더 커진 L3 캐시를 통해 데이터 처리 속도를 높였습니다.

<br/>
<br/>

### Big Tech Custom ARM Silicon Trends (빅테크 독자 칩 개발 트렌드)

미국의 주요 빅테크 기업들은 범용 CPU(Intel/AMD) 의존도를 낮추고, 자사 클라우드 및 AI 워크로드에 최적화된 독자적인 ARM 칩을 개발하는 추세입니다.

- **배경**: AI 및 대규모 클라우드 워크로드 처리에 있어 **전력 효율(Energy Efficiency)**과 **가성비(Price-Performance)**가 핵심 경쟁력이 되었기 때문입니다.

- **주요 기업 현황**:

1. **Google Cloud (Axion)**: 2024년 발표된 구글 최초의 데이터센터용 ARM CPU입니다. 기존 x86 인스턴스 대비 50% 이상의 성능 향상과 60% 이상의 에너지 효율을 제공합니다.

2. **Microsoft Azure (Cobalt)**: Azure에 최적화된 'Cobalt 100' 및 후속 모델 'Cobalt 200'을 개발하여, VM 및 SQl DB 서비스 등에 적용하고 있습니다.

3. **Meta/Apple/NVIDIA**: Meta는 AI 가속기(MTIA)에 집중하고 있으며, NVIDIA는 Grace CPU를 통해 고성능 ARM 서버 시장을 리딩하고 있습니다.

- **시사점**: 쿠버네티스 스케줄러 입장에서는 `nodeAffinity`나 `tolerations`를 통해 이러한 ARM 노드(`kuberentes.io/arch=arm64`)에 적절한 워크로드를 배치하는 전략이 더욱 중요해지고 있습니다.

<br/>
<br/>
  

# 스케쥴링 프로세스 (스케쥴링 과정)

스케줄러가 Pod를 노드에 배치하기까지의 5단계 과정입니다.

<br/>

## 1. Pod Admission (파드 수신 및 대기)

- 스케줄러는 스케줄링할 준비가 된(`Pending` 상태이고 `nodeName`이 없는) Pod를 큐(Queue)에서 꺼내 수신합니다.

- Pod의 리소스 요구 사항(CPU/MEM Requests) 및 초기 제약 조건(Pvc 등)을 확인하여 기본적으로 배치 가능한지 검토합니다.

<br/>
<br/>
  

## 2. Node Filtering (노드 필터링)

- 클러스터의 모든 노드 중에서 **"이 Pod를 실행할 자격이 되는 노드"**만 추려냅니다(예선전).

- **고려 사항**:

- **리소스**: CPU/Memory 가용량이 Pod의 Request를 감당할 수 있는가?

- **제약 조건**: Taint 가 걸려있는가? Node Selector 라벨이 일치하는가?

- 자격이 없는 노드는 후보군에서 제외됩니다. (만약 남은 노드가 0개면 스케줄링 실패 -> Pending 유지)

<br/>
<br/>

## 3. Node Scoring (노드 스코어링)

- 필터링을 통과한 후보 노드들에게 **점수(0~100점)**를 매깁니다(본선).

- **채점 기준(함수)**:

- **Node Utilization**: 리소스 여유가 많은 곳이 좋은가(LeastRequested)? 아니면 꽉 채우는 게 좋은가(MostRequested)?

- **Affinity**: Pod Affinity(친한 파드)가 있는 노드에 가산점.

- **Image Locality**: 이미지가 이미 다운로드된 노드에 가산점.

<br/>
<br/>
  

## 4. Node Selection (노드 선택)

- 가장 높은 점수를 받은 **최적의 노드 1개**를 최종 선택합니다.

- 만약 최고점이 동점인 노드가 여러 개라면, Round-Robin 방식 등을 사용하여 하나를 무작위로 선택합니다.

<br/>
<br/>


## 5. Binding (바인딩)

- 최종 선택된 노드에 Pod를 **할당(Bind)** 합니다.

- 스케줄러는 API Server에게 "이 Pod의 `nodeName` 필드를 `Node A`로 업데이트해줘"라고 요청합니다.

- 이후 해당 노드의 Kubelet이 이를 감지하여 컨테이너 생성을 시작합니다.

<br/>
<br/>
  

# Scheduling Algorithms and Policies (스케줄링 알고리즘 및 정책)

스케줄러가 최적의 노드를 선정하기 위해 사용하는 주요 알고리즘과 정책들입니다.<br/>
<br/>


## 1. Node Resource Fit (노드 리소스 적합성)

- 스케줄러는 각 노드의 **사용 가능한 리소스(CPU, Memory 등)**를 평가합니다.

- Pod가 요구하는 리소스 용량(Requests)을 충족하지 못하는 노드는 배제하고, 요구 사항에 가장 잘 맞는 노드를 선택합니다.

<br/>
<br/>

## 2. Affinity and Anti-Affinity (친화성 및 반친화성)

- **Affinity**: 특정 라벨을 가진 노드나 Pod 근처에 배치되도록 유도합니다. (예: Web 서버와 DB를 가깝게)

- **Anti-Affinity**: 특정 노드나 Pod와 멀어지도록 배치합니다. (예: 이중화된 Pod가 같은 노드에 겹치지 않게)

<br/>
<br/>

## 3. Taints and Tolerations (테인트 및 톨러레이션)

- 노드에 **Taint(출입 금지)**를 설정하여 특정 Pod만 들어오게 하거나 막습니다.

- Pod에 **Toleration(통행증)**이 있어야만 해당 노드에 스케줄링 될 수 있습니다. (예: GPU 노드는 GPU Pod만 쓰도록 제한)

<br/>
<br/>  

## 4. Node Selector (노드 선택자)

- 가장 기본적인 배치 방식으로, Pod 사양에 `nodeSelector`를 정의하여 특정 라벨(`disk=ssd`)을 가진 노드를 직접 지정하여 찾습니다.

<br/>
<br/>

## 5. Prioritization (우선순위 지정)

- 필터링된 후보 노드들 사이에서 순위를 매기는 과정입니다.

- 다양한 **우선순위 함수(Priority Functions)**를 적용하여 점수를 계산하고, 가장 높은 점수를 얻은 노드가 최종 선택됩니다.

<br/>
<br/>
<br/>
<br/>