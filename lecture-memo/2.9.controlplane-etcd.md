# 2.9.controlplane - etcd 

> AI 가 정리해서 서식이 좀 깨지는 부분이 있는데 목요일 중으로 수정 예정. AI 는 이건 또 못하네...

강의에서 첨부한 자료가 없어서 직접 추가
![](https://kubernetes.io/images/docs/kubernetes-cluster-architecture.svg)

출처 : https://kubernetes.io/docs/concepts/architecture/
- control plane 검색하면 나옴
- 또는 https://kubernetes.io/docs/concepts/overview/components/ 에서 설명하는 그림 역시 좋다.


그런데 node 가 많아지고, 쿠버네티스 api-server 가 처리해야 할 데이터가 많아지면 etcd 의 볼륨이 엄청나게 커진다.<br/>
<br/>

etcd 
- key/value 스토어
- api-server 만 접근 가능(위의 그림을 자세히 보자)
- **분산** 데이터 저장소 (데이터의 복제본을 보유하고, Fail Over 를 통해 1기가 망가져도 다른 인스턴스를 통해 접근)
- 일관적, 고가용성 키/값 저장
	- 난 이글을 읽을 때 etcd 에서 데이터의 일관성도 지원한다는 점이 새로웠었다.

<br/>
<br/>

다음 내용부터는 강의 내용을 gemini 와 함께 자료를 찾고 질문해서 요약한 내용이다.<br/>
로컬 파일에는 k8s-notes/fastcampuse-k8s/2.9-etcd-and-controplane.md 에 원본으로 저장해두었다.<br/>
<br/>

쿠버네티스를 직접 운영할 경우는 어렵다. 줄었다 늘였다 이게 어렵다. Cloud Managed 쿠버네티스는 이용이 편리하다. 물론 eks,gke 등의 시스템 내에는 동적으로 줄었다 늘였다 하는 기준이 있을거라고 함

<br/>
<br/>

# etcd와 Control Plane 개요
## 1. Distributed Key-Value Store (분산 키-값 저장소)

etcd는 Kubernetes의 백본 역할을 하는 분산형, 일관적이고 고가용성 키-값 저장소입니다. 클러스터의 모든 상태와 구성 데이터를 저장하고 관리하는 핵심 구성 요소입니다.

<br/>
<br/>

## 2. Scalability and Performance (확장성 및 성능)

etcd는 높은 확장성을 갖도록 설계되었습니다. 수천 개의 노드와 수백만 개의 키를 처리할 수 있으며, 이를 통해 쿠버네티스가 대규모 동적 환경을 효율적으로 관리할 수 있도록 보장합니다.

<br/>
<br/>  

## 3. Cluster Orchestration (클러스터 조율)

쿠버네티스는 etcd를 사용하여 노드 상태, Pod 세부 정보, 서비스 구성과 같은 중요한 클러스터 정보를 저장합니다. 쿠버네티스는 이 정보를 바탕으로 전체 클러스터를 조율하고 관리합니다.

<br/>
<br/>  

## 4. High Availability (고가용성)

etcd는 다중 노드 복제 아키텍처를 통해 높은 가용성을 제공합니다. 개별 etcd 노드에 장애가 발생하더라도 Kubernetes 시스템이 중단 없이 계속 작동할 수 있도록 지원합니다.

<br/>
<br/> 

## 5. Reliability and Consistency (신뢰성 및 일관성)

etcd는 강력한 일관성 보장과 장애 내구성을 제공합니다. 덕분에 Kubernetes는 네트워크 분할이나 노드 장애가 발생하는 상황에서도 클러스터 상태에 안정적으로 액세스하고 업데이트할 수 있습니다.

<br/>
<br/>  

## 6. Raft Consensus Algorithm (Raft 합의 알고리즘)

etcd는 분산 시스템의 일관성을 유지하기 위해 **Raft 알고리즘**을 사용합니다.

- **Raft란?**: 여러 노드가 동일한 데이터를 유지하도록 보장하는 합의 프로토콜입니다. 리더(Leader)를 선출하고, 리더를 통해 모든 데이터 변경 사항을 복제(Replication)하여 일관성을 맞춥니다.

- **3, 5, 7 노드 구성의 이유**: Raft 알고리즘은 **과반수(Quorum)**의 동의가 있어야 결정을 내릴 수 있습니다.

- **Quorum**: `(N / 2) + 1` (N은 전체 노드 수)

- **홀수 구성을 권장하는 이유**:

- 짝수(예: 4개)일 경우, 네트워크 분할 시 2:2로 표가 갈리면 과반수를 넘지 못해 멈추는(Split Brain) 상황이 발생할 수 있습니다.

- 홀수(예: 3개)일 경우, 1개가 죽어도 2개가 남아 과반(2 > 1.5)을 충족하므로 서비스 유지가 가능합니다.

- **장애 허용 (Fault Tolerance)**:

- 3개 노드: 1개 장애 허용

- 5개 노드: 2개 장애 허용

- 7개 노드: 3개 장애 허용

<br/>
<br/>  

## 7. Comparison with Consul (Consul과의 비교)

네, **Consul**도 etcd와 매우 유사한 컨셉과 아키텍처를 가지고 있습니다.

- **공통점**:

- **Raft 알고리즘 사용**: Consul 역시 데이터 일관성을 위해 Raft 프로토콜을 사용합니다.

- **홀수 노드 권장**: Consul의 **Server Agent**들도 3대 또는 5대로 구성하는 것을 권장하며, 이유는 etcd와 동일하게 과반수(Quorum) 유지와 Split Brain 방지 때문입니다.

- **기능**: 분산 Key-Value 저장소 기능을 제공하며, 고가용성을 목표로 합니다.

- **Watch 기능 지원**: etcd와 마찬가지로 **Watch 기능**을 제공합니다. 특정 키(Key)나 서비스 상태가 변경되면 이를 감지하여 알림을 주거나 스크립트를 실행할 수 있습니다.

- **차이점**:

- **etcd**: Kubernetes의 **메타데이터 저장**에 특화되어 있으며, 단순하고 빠릅니다. **gRPC 기반의 Watch**를 통해 변경 사항을 실시간 스트림으로 클라이언트에 푸시합니다.

- **Consul**: **서비스 디스커버리(Service Discovery)**, 헬스 체크, 멀티 데이터센터 지원 등 더 광범위한 **서비스 메쉬** 기능을 포함한 솔루션입니다. Watch 기능은 주로 **블로킹 쿼리(Blocking Query)**나 외부 핸들러(스크립트/HTTP) 실행 방식으로 동작합니다.

<br/>
<br/>  

## 8. Data Consistency Example (데이터 일관성 예시)

etcd의 데이터 일관성이란 **"누가 언제 조회하더라도 항상 최신의 동일한 데이터를 보장하는 것"**입니다.<br/>
<br/>

**예시 시나리오**: 사용자 A가 `key=count`, `value=5`라고 저장 요청을 보냈을 때<br/>  

1. **Leader 수신**: 클라이언트의 요청(`count=5`)은 무조건 **Leader 노드**가 받습니다.

2. **로그 복제 (Log Replication)**: Leader는 즉시 저장하지 않고, 다른 **Follower 노드들(3대 중 2대)**에게 "이 값을 저장해도 될까?"라고 로그를 보냅니다.

3. **과반수 합의 (Quorum)**: Follower들 중 과반수 이상(예: 3대 중 2대)이 "저장했다"고 응답(Ack)을 보내면, 그때 Leader는 데이터를 **확정(Commit)** 짓습니다.

4. **응답**: 그 후 사용자에게 "저장 완료"를 알립니다.

<br/>
<br/>

**결과**:<br/>
- 사용자 B가 바로 0.001초 뒤에 다른 Follower 노드에게 `count` 값을 물어봐도, 이미 합의가 끝났기 때문에 무조건 `5`라는 **최신 값**을 받게 됩니다.
- (비교: 일관성이 낮은 시스템은 Leader에는 `5`가 저장됐지만, 아직 전파가 안 돼서 복제본(Follower)에서 `4`를 읽을 수도 있습니다. etcd는 이런 일이 발생하지 않음을 보장합니다.)

<br/>
<br/>
  

## 9. Watch Mechanism & Scheduler Workflow (감지 매커니즘 및 스케줄러 절차)
Kubernetes에서 상태 변경을 감지하고 전파하는 핵심 원리는 **"API Server 중심의 Watch 체인"**입니다.

<br/>
<br/>

### 9.1. 누가 etcd를 Watch 하는가? (The Only Direct Watcher)
- **오직 API Server만이 etcd를 직접 Watch 합니다.**
- 보안 및 아키텍처 단순화를 위해, Scheduler나 Kubelet 같은 다른 모든 컴포넌트는 etcd에 직접 접근할 수 **없습니다**.

<br/>
<br/>  

### 9.2. 다른 컴포넌트(Scheduler 등)는 어떻게 변경을 아는가?
- 모든 컴포넌트(Scheduler, Controller Manager, Kubelet)는 **API Server를 Watch** 합니다.

- **ResourceVersion**이라는 값을 통해, 마지막으로 확인한 시점 이후의 변경 사항만 스트림으로 지속적으로 전송받습니다.

<br/>
<br/>  

### 9.3. 상세 작동 절차 (Step-by-Step)
**상황**: 사용자가 새로운 Pod를 생성했을 때, Scheduler가 이를 감지하고 노드를 할당하는 과정

<br/>
<br/>  

1. **변경 발생 (Change Initiated)**: 사용자가 `kubectl run nginx` 명령을 실행하면, API Server가 이를 받아 `etcd`에 Pod 정보를 **저장**합니다.

- 이때 Pod의 `nodeName` 필드는 비어 있습니다 (Pending 상태).

2. **API Server 인지**: `etcd`에 저장이 완료되면, API Server는 내부적으로 "새로운 Pod가 생겼음"을 인지합니다.

3. **Scheduler 감지 (Watch Event)**:

- **Scheduler**는 항상 API Server에 **"Pod 중에 `nodeName`이 없는 녀석이 생기면 즉시 알려줘(Watch)"**라고 요청해 둔 상태입니다.

- API Server는 이 조건에 맞는 이벤트가 발생하자마자 Scheduler에게 **"새 녀석(Event: ADDED) 등장!"**이라고 알림을 보냅니다.

4. **스케줄링 수행 (Scheduling)**:

- 알림을 받은 Scheduler는 알고리즘을 돌려 적절한 노드(예: `Node-A`)를 선택(Binding)합니다.

- Scheduler는 다시 API Server에게 **"이 Pod를 Node-A에 할당해(Update)"**라고 요청을 보냅니다.

5. **상태 업데이트**: API Server는 이 정보를 다시 `etcd`에 저장하고, 이번에는 `Kubelet`(Node-A에 있는)이 이를 감지하게 됩니다.

<br/>
<br/> 

## 10. Self-Hosted vs. Managed Kubernetes (직접 구축 vs. 클라우드 매니지드)

**결론: etcd를 포함한 컨트롤 플레인의 동적 확장을 직접 구현하는 것은 매우 어렵고 위험합니다.**

<br/>
<br/>  

### 10.1. 직접 구축(Self-Hosted) 시의 어려움

사용자가 직접 "Scalability"를 위해 etcd나 마스터 노드를 동적으로 늘리고 줄이는 시스템을 구축하려면 다음과 같은 난관에 부딪힙니다.
<br/>

1. **etcd의 까다로운 관리**:

- etcd는 **Quorum(과반수)** 유지가 생명입니다. 부하가 늘었다고 갑자기 노드를 하나 툭 추가하면, 데이터 동기화(Rebalancing)와 멤버십 변경 과정에서 전체 클러스터가 일시적으로 불안정해질 수 있습니다.

- 따라서 etcd는 보통 **3대 또는 5대로 고정**해두고 쓰지, 트래픽에 따라 오토스케일링하지 않습니다.

2. **Control Plane의 복잡성**:

- 마스터 노드를 늘리려면 로드밸런서 설정, 인증서(CA, API Server Cert) 재배포, etcd 멤버십 재구성 등 엄청난 운영 오버헤드가 발생합니다.

3. **Worker Node 확장 (Cluster Autoscaler)**:

- 워커 노드 확장은 그나마 낫지만, 이를 위해서도 클라우드 인프라(AWS ASG 등)와 연동되는 **Cluster Autoscaler**를 직접 설치하고 권한을 설정해야 합니다.

<br/>
<br/>

### 10.2. 클라우드 매니지드(EKS, GKE, AKS)를 쓰는 이유

매니지드 서비스를 사용하면 **"Control Plane(etcd 포함)에 대한 고민을 돈으로 해결"**할 수 있습니다.<br/>
<br/>

1. **Control Plane 완전 관리 (Black Box)**:

- AWS EKS 같은 서비스는 **Control Plane(API Server + etcd)**을 사용자가 아예 관리하지 않도록 숨겨둡니다.

- AWS가 알아서 고성능 인스턴스를 할당하고, etcd 백업, 패치, 가용성 유지, 스케일링을 전담합니다. 사용자는 etcd가 터질까 걱정할 필요가 **전혀 없습니다.**

2. **손쉬운 노드 확장**:

- 워커 노드 부하가 늘어나면 체크박스 몇 개나 간단한 설정(Node Group)만으로 **Cluster Autoscaler**가 작동하여 자동으로 EC2 인스턴스를 늘리고 줄여줍니다.

- 즉, 사용자는 etcd의 일관성이나 마스터 노드의 부하를 신경 쓸 필요 없이, **"내 애플리케이션이 잘 돌아가는가"**에만 집중하면 됩니다.

<br/>
<br/>

## 11. Who performs Cluster Orchestration? (클러스터 조율의 주체)
앞선 **[3. Cluster Orchestration]** 섹션에서 *"쿠버네티스는 etcd를 사용하여... 전체 클러스터를 조율하고 관리합니다"*라고 언급했습니다.

여기서 말하는 **"쿠버네티스"**라는 추상적인 개념을 실제로 수행하는 구체적인 컴포넌트는 **Controller Manager**와 **Scheduler**입니다.

<br/>
<br/>  

### 11.1. 섹션 3의 상세 해석
섹션 3의 설명은 데이터의 **"저장소(etcd)"** 관점에서 서술된 것입니다. 이를 **"행위자(Actor)"** 관점에서 다시 풀면 다음과 같습니다.<br/>
<br/>
  

1. **"정보를 저장하고" (etcd & API Server)**:

- 모든 상태 정보(Pod가 몇 개여야 하는지, 현재 몇 개인지 등)는 `etcd`에 저장됩니다.

- 이 정보를 조회하고 쓸 수 있는 유일한 통로는 `API Server`입니다.

2. **"조율하고 관리합니다" (Controller Manager & Scheduler)**:

- **Controller Manager**: `etcd`에 저장된 **"희망 상태(Desired State)"**를 끊임없이 확인합니다. 만약 "Pod 3개를 유지하라"고 적혀있는데 "현재 2개"라면, 즉시 "1개 더 생성하라"는 명령을 내립니다. 이것이 바로 **Orchestration(조율)**의 핵심입니다.

- **Scheduler**: 새로 생성된 Pod가 어느 노드에 갈지 결정되지 않았다면, `etcd`의 노드 상태 정보를 보고 적절한 곳에 배치합니다.

  

**요약하자면**:

- **etcd**: 악보 (조율을 위한 데이터)

- **API Server**: 지휘자 (정보 전달 및 명령 하달)

- **Controller Manager**: 연주자 (실제로 소리를 내고 맞추는 행위자)

<br/>
<br/>

따라서 **"클러스터 조율"**은 etcd에 저장된 데이터를 근거로 **Controller Manager가 끊임없이 상태를 일치시키는 과정**을 의미합니다.

<br/>
<br/>

### 11.2. 모든 명령은 API Server를 경유 (All Roads Lead to API Server)

**네, 맞습니다. Controller Manager는 절대 직접 명령을 내리지 못합니다.**

- Controller Manager가 "Pod를 하나 더 만들어야겠다"고 판단하더라도, 직접 워커 노드의 Kubelet에게 "야, 만들어!"라고 명령할 권한도, 연결 통로도 없습니다.

- **반드시 API Server에게 요청서(Object Create Request)를 제출해야 합니다.**

1. **Controller Manager**: "API Server님, Pod 하나 생성하는 내용으로 etcd에 좀 적어주세요." (API Call)

2. **API Server**: "알겠음. etcd에 적었음."

3. **Kubelet (Worker Node)**: (API Server를 Watch하고 있다가) "어? 내 노드에 새 Pod 할일이 적혔네? 생성 시작!"

- 즉, Controller Manager의 "명령"이란 사실상 **"API Server에 희망 상태(Desired State) 데이터를 업데이트하는 행위"**입니다.

<br/>
<br/>  

### 11.3. etcd 접근 권한 (Only API Server)

**네, 맞습니다. 쿠버네티스 아키텍처에서 etcd에 접근할 수 있는 유일한 컴포넌트는 API Server뿐입니다.**

  

다른 컴포넌트(Controller, Scheduler, Kubelet, `kubectl` 사용자)는 절대 etcd에 직접 접속할 수 없으며, 반드시 API Server를 거쳐야 합니다. 그 이유는 다음과 같습니다.

  

1. **보안 및 인증 (Authentication & Authorization)**:

- API Server는 "누가 요청했는지(User)", "권한이 있는지(RBAC)"를 철저히 검사하는 문지기입니다.

- 만약 누구나 etcd에 직접 붙을 수 있다면, 권한 검사 없이 데이터를 조작할 수 있어 보안상 치명적입니다.

2. **데이터 무결성 검증 (Validation)**:

- API Server는 데이터가 etcd에 저장되기 전에 **유효성 검사**를 수행합니다. (예: "Pod 이름에 특수문자가 들어갔나요?", "필수 필드가 빠졌나요?")

- etcd는 단순한 저장소이므로 이런 로직을 모릅니다. 직접 쓰면 엉망인 데이터가 저장될 수 있습니다.

3. **일관된 인터페이스 (Abstraction)**:

- 내부 저장소가 etcd에서 SQL DB로 바뀌더라도, 다른 컴포넌트들은 알 필요가 없습니다. API Server가 중간에서 추상화해주기 때문입니다.

  

## 12. How High Availability Works (고가용성 작동 원리)

앞선 **[4. High Availability]** 섹션에서 언급한 *"다중 노드 복제 아키텍처를 통한 고가용성"*이 실제로 어떻게 작동하는지 설명합니다. 핵심은 **"리더 선출(Leader Election)"**과 **"데이터 복제(Replication)"**입니다.

  

### 12.1. 리더 선출을 통한 자동 복구

etcd 클러스터는 항상 **1명의 Leader**와 **나머지 Follower**로 구성됩니다.

- **장애 상황**: 만약 현재 Leader 노드가 죽으면(네트워크 단절, 파워 오프 등), Follower들은 "어? 리더한테서 연락(Heartbeat)이 안 오네?"라고 감지합니다.

- **선거 시작**: 남은 Follower들은 즉시 **새로운 투표**를 시작합니다.

- **새 리더 선출**: 과반수(Quorum)의 동의를 얻은 Follower가 **새로운 Leader**로 승격됩니다.

- **결과**: 이 모든 과정은 수 초 이내에 자동으로 일어나며, API Server는 잠시 재시도(Retry)를 하다가 새 Leader에게 연결되므로 서비스는 중단되지 않습니다.

  

### 12.2. 데이터 복제 (Replication)

고가용성의 또 다른 축은 **데이터가 사라지지 않는 것**입니다.

- etcd는 어떤 데이터가 들어오면, Leader 혼자만 갖고 있는 게 아니라 **모든 Follower에게 강제로 복사**해 둡니다.

- 따라서 Leader가 갑자기 불타 없어져도, 이미 다른 Follower들이 똑같은 데이터를 가지고 있기 때문에 데이터 유실 없이 서비스를 계속할 수 있습니다.

  

## 13. Deep Dive into Reliability & Consistency (신뢰성과 일관성 심화)

**[5. Reliability and Consistency]** 섹션에서 언급된 "네트워크 분할이나 노드 장애 상황"에서의 동작 방식을 구체적으로 설명합니다.

  

### 13.1. Reliability (신뢰성) - 데이터는 절대 사라지지 않는다

"신뢰성"이란 시스템이 갑자기 꺼져도(Power Failure) 데이터가 안전하게 보존됨을 의미합니다. etcd는 **WAL(Write Ahead Log)** 기술을 사용합니다.

- **선 기록 후 처리**: etcd는 메모리에 데이터를 쓰기 전에, 디스크의 로그 파일(WAL)에 먼저 "나 이거 쓸 거야"라고 기록합니다.

- **장애 복구**: 정전으로 서버가 꺼졌다가 다시 켜지면, etcd는 이 WAL 파일을 읽어서 "아, 내가 쓰려다가 못 쓴 데이터가 있네?" 하고 복구(Replay)합니다. 이 덕분에 데이터 유실이 없습니다.

  

### 13.2. Consistency in Network Partition (네트워크 분할 시의 일관성)

네트워크 문제로 클러스터가 두 그룹으로 쪼개지는(Partition) 상황에서도 etcd는 **일관성(Consistency)**을 최우선으로 지킵니다. (CAP 이론의 **CP 시스템**)

- **상황**: 5대(A, B, C, D, E)가 있는데 네트워크가 끊겨서 `(A, B)` / `(C, D, E)`로 나뉨.

- **동작**:

- **(A, B) 그룹**: 2명밖에 없으므로 과반수(3명)가 안 됩니다. 이쪽은 **쓰기 작업을 아예 거부(Stop)**합니다. (잘못된 데이터 저장을 방지)

- **(C, D, E) 그룹**: 3명이므로 과반수를 충족합니다. 이쪽 그룹만 정상적으로 작동하여 데이터를 계속 업데이트합니다.

- **결과**: 나중에 네트워크가 복구되면 (A, B)는 그동안 (C, D, E)가 업데이트한 내용을 받아와서 다시 하나가 됩니다.

- **의의**: "잠시 멈출지언정, 서로 다른 데이터(Split Brain)를 갖지는 않겠다"는 철학으로 쿠버네티스 클러스터의 데이터가 꼬이는 것을 원천 차단합니다.

  

## 14. Key Functions of etcd in Kubernetes (etcd의 핵심 기능)

이미지에서 설명하는 etcd가 쿠버네티스 내에서 수행하는 5가지 주요 역할입니다.

  

1. **Cluster State (클러스터 상태)**

- Pod, Service, Namespace 등 **모든 리소스의 구성 정보**와 **클러스터의 현재 상태**를 저장합니다.

- 쿠버네티스의 "기억(Memory)"을 담당합니다.

2. **Resource Definitions (리소스 정의)**

- 모든 쿠버네티스 리소스의 **Desired State(희망 상태)**를 저장합니다.

- 메타데이터와 설정(Configuration) 파일의 원본이 보관되는 곳입니다.

3. **Cluster Authentication and Authorization (클러스터 인증 및 권한)**

- API Server가 접근 제어를 수행할 때 참조하는 **인증(User) 및 권한(RBAC Roles)** 정보를 저장합니다.

- 이를 통해 클러스터에 대한 안전한 액세스를 보장합니다.

4. **Distributed Locking and Leader Election (분산 잠금 및 리더 선출)**

- 쿠버네티스 핵심 컴포넌트(Scheduler, Controller Manager)들이 동시에 여러 개 뜰 때, 누가 대장인지 정하는 **리더 선출(Leader Election)** 메커니즘을 제공합니다.

- 이를 통해 고가용성(HA) 환경에서도 충돌 없이 오직 한 놈만 일하도록(Coordination) 보장합니다.

5. **Kubernetes Events (쿠버네티스 이벤트)**

- 클러스터에서 일어나는 모든 사건(**Event**: Pod 생성, 에러, 노드 재시작 등)을 기록합니다.

- 이는 모니터링, 디버깅, 문제 해결(Troubleshooting) 시 중요한 로그 역할을 합니다.

  
  

요약

1. Cluster State: 현재 클러스터가 어떻게 생겼는지 저장 (Pod, SVC 등).

2. Resource Definitions: 사용자가 원하는 설정(YAML 원본) 저장.

3. Authentication & Authorization: 보안 접속을 위한 ID/비밀번호/권한 저장.

4. Distributed Locking: 여러 컴포넌트 중 '리더'만 일하도록 교통정리.

5. Events: 발생한 사건/사고 로그 저장 (디버깅용).

  

### 14.1. 상세 설명: Cluster State (클러스터 상태)

**"Cluster State"**란 쿠버네티스 클러스터 안에 존재하는 **모든 것의 '존재 증명'이자 '현재 상황'**입니다. etcd는 이 상태를 저장하는 **유일한 진실 공급원(Single Source of Truth)**입니다.

  

1. **무엇을 저장하는가? (Scope)**

- **인프라 정보**: 어떤 노드(Node)가 연결되어 있고, 각 노드의 IP와 리소스(CPU/RAM) 상태는 어떤지.

- **워크로드 정보**: 현재 실행 중인 Pod는 무엇이며, 어디에 떠 있고, IP는 무엇인지.

- **네트워크 정보**: Service IP, Endpoints(연결된 Pod 목록), Ingress 규칙 등.

- **설정 정보**: ConfigMap, Secret 등 애플리케이션 설정값.

  

2. **Spec(명세) vs Status(상태)**

etcd에 저장된 객체(Object) 정보는 크게 두 부분으로 나뉘어 "상태"를 더욱 정교하게 관리합니다.

- **Spec (Desired State)**: 사용자가 "이렇게 해주세요"라고 요청한 내용 (예: `replicas: 3`).

- **Status (Current State)**: 시스템이 확인한 "실제 상황" (예: `readyReplicas: 2` - 하나가 아직 안 떴음).

- **중요성**: etcd는 이 두 가지를 모두 저장합니다. 컨트롤러는 `Spec`과 `Status`가 달라지면(Diff), 그것을 일치시키기 위해 움직입니다.

  

3. **"기억(Memory)"의 의미**

- 만약 etcd 데이터가 날아가면, 멀쩡히 돌아가던 컨테이너들은 "고아(Orphan)"가 됩니다.

- API Server는 자신이 관리하던 Pod가 누군지, Service가 어디로 연결돼야 하는지 **모두 잊어버리게** 됩니다(Amnesia).

- 따라서 **"etcd의 상태 = 클러스터 그 자체"**라고 할 수 있습니다.

  

### 14.2. 상세 설명: Resource Definitions (리소스 정의)

**"Resource Definitions"**는 사용자가 쿠버네티스에게 바라는 **"주문서 원본"**을 의미합니다. 단순히 현재 상태(Status)를 넘어서, 사용자의 **의도(Intent)**가 영구적으로 보관되는 곳입니다.

  

1. **YAML 파일의 종착지**

- 사용자가 `kubectl apply -f my-app.yaml`을 실행하면, 이 YAML 파일의 내용이 JSON으로 변환되어 etcd에 저장됩니다.

- 즉, 우리가 작성한 `Deployment`, `Service`, `ConfigMap`의 명세서가 파일 시스템이 아닌 **etcd 키-값 저장소**에 영구 보존됩니다.

  

2. **선언적 구성(Declarative Configuration)의 핵심**

- 쿠버네티스는 "명령형(Imperative)"이 아닌 "선언적(Declarative)" 시스템입니다.

- etcd는 **"나는 웹서버 3개를 원해"**라는 사용자의 선언(Declaration)을 저장해 둡니다.

- 설령 모든 웹서버가 다 죽더라도, etcd에 이 **"정의(Definition)"**가 남아있는 한, 컨트롤러는 계속해서 웹서버를 되살려냅니다. (이것이 Self-Healing의 원천입니다.)

  

3. **메타데이터의 보관소**

- 리소스의 실제 설정뿐만 아니라, 관리를 위한 꼬리표들도 저장됩니다.

- **Labels**: 리소스를 검색하고 분류하기 위한 이름표 (예: `app=nginx`).

- **Annotations**: 시스템이나 외부 도구가 사용하는 주석 (예: `deployment.kubernetes.io/revision`).

- **OwnerReferences**: 누가 이 리소스를 만들었는지 족보 관리 (예: Pod는 ReplicaSet이 만들었음).

  

### 14.3. 상세 설명: Cluster Authentication and Authorization (인증 및 권한)

etcd는 누가(Who) 클러스터에 들어올 수 있고, 무엇을(What) 할 수 있는지에 대한 보안 정책의 **저장소**입니다.

  

1. **Service Accounts & Secrets**

- Pod가 API Server와 통신할 때 사용하는 **ServiceAccount 토큰**이 etcd에 Secret 형태로 안전하게 암호화되어 저장됩니다.

2. **RBAC (Role-Based Access Control)**

- "개발자 김씨는 Pod를 읽을 수만 있고(Reader), 운영자 이씨는 삭제도 할 수 있다(Admin)" 같은 **역할(Role)**과 **권한 부여(Binding)** 규칙이 모두 etcd에 저장됩니다.

- API Server는 요청이 들어올 때마다 etcd에 있는 이 규칙표를 확인하여 승인/거부 여부를 결정합니다.

  

### 14.4. 상세 설명: Distributed Locking and Leader Election (분산 잠금 및 리더 선출)

여러 대의 서버가 동시에 돌아가는 분산 환경(HA)에서 서로 엉키지 않게 교통정리를 하는 기능입니다.

  

1. **왜 필요한가? (The Problem)**

- 고가용성을 위해 **Controller Manager**를 3대 띄웠다고 가정합시다.

- Pod 하나가 죽었을 때, 3명의 컨트롤러가 동시에 "어? 내가 살려야지!" 하고 달려들면 Pod가 3개 생성되는(중복) 문제가 생깁니다.

2. **어떻게 동작하는가? (The Solution via etcd)**

- 컨트롤러들은 etcd에 있는 특정 키(Lock)를 먼저 차지하려고 경쟁합니다 (Atomic Operation).

- **Lease(임대) 메커니즘**: 가장 먼저 성공한 1명만 **Leader**가 되어 실제로 일을 하고, 나머지는 **Standby** 상태로 대기합니다.

- 리더는 주기적으로 etcd에 "나 살아있어"라고 갱신(Renew)하며, 만약 리더가 죽으면 대기하던 다른 놈이 즉시 키를 차지하고 새 리더가 됩니다.

  

### 14.5. 상세 설명: Kubernetes Events (쿠버네티스 이벤트)

**Event**는 클러스터에서 일어나는 사건/사고를 기록한 일종의 **"블랙박스 로그"**입니다. 주의할 점은, 로그 파일이 아니라 이것도 **etcd 객체(Object)**라는 점입니다.

  

1. **특징**

- `kubectl get events` 또는 `kubectl describe pod` 명령어로 볼 수 있는 내용들입니다.

- 예: `Pulling image...`, `Successfully assigned...`, `Back-off restarting failed container...`

2. **1시간의 수명 (TTL)**

- 이벤트 데이터는 너무 많이 쌓이면 etcd 성능을 저하시킬 수 있습니다.

- 따라서 쿠버네티스는 기본적으로 이벤트 객체를 생성 후 **1시간 뒤에 자동 삭제**되도록 설정하여 etcd 용량을 관리합니다.

3. **디버깅의 핵심**

- "내 Pod가 왜 안 뜨지?" 할 때 가장 먼저 봐야 하는 것이 바로 이 etcd에 저장된 Event 정보입니다. Error의 원인이 상세히 적혀있기 때문입니다.


